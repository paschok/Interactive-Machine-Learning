{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31658,
     "status": "ok",
     "timestamp": 1606772359937,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "HxUHiqAr4A4f",
    "outputId": "1cf0ead4-1647-4b75-aedd-2d061d5a6c30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdUxoN773z22"
   },
   "source": [
    "# Interactive Machine Learning - Exercise 03\n",
    "\n",
    "In this exercise we will learn about cooperative machine learning.\n",
    "Our goal is it to build a very basic cooperative machine learning user interface and use it to extend our Pokedex model from the last exercise.\n",
    "\n",
    "The steps you are going to cover are as follows:\n",
    "* Pretrain our Pokedex model with the original data\n",
    "* Manually label a small bit of new data\n",
    "* Train our model on the new data\n",
    "* Use the model in a cooperative workflow to annotate the rest of the dataset\n",
    "\n",
    "Please read each exercise carefully before you start coding! You will find a number in the comments before each step of coding you will do. Please refer to these numbers if you have any questions.\n",
    "\n",
    "## 0. Import the libraries\n",
    "As always we are providing a list useful packages in the import section below.\n",
    "Keep in mind that you can import additional libraries at any time and that you do not need to use all the imports if you know another solution for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1606772363429,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "NYZvDW-23z22",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from IPython.display import Image\n",
    "from ipywidgets import interact_manual, interact\n",
    "\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36QCCyEu3z22"
   },
   "source": [
    "## 1. Pretrain the model\n",
    "In this part we are going to pretrain our model on the pokemon images you already know.\n",
    "To this end we will use the same VGG16 model as last week with the following training procedure:\n",
    "\n",
    "Preprocessing:\n",
    "* Imagesize (224,224)\n",
    "* Vgg16 standard preprocessing from the Keras framework\n",
    "\n",
    "Datasplit:\n",
    "* Use 90% of the data to train and 10% to valitdate your results\n",
    "\n",
    "Training 1:\n",
    "* Initialize the model with the imagenet weights\n",
    "* Freeze all convolution layers\n",
    "* Train the model using the following settings:\n",
    " * 5 Epochs\n",
    " * Adam Optimizer with default Parameters\n",
    " * categorical cross entropy loss\n",
    " * Batchsize 32\n",
    "\n",
    "Training 2:\n",
    "*  Unfreeze the last two convolutional Blocks\n",
    "*  Continue training with the following settings:\n",
    " * 10 Epochs\n",
    " * Adam Optimizer with a learning rate of 0.0001\n",
    " * Batchsize of 32\n",
    "\n",
    "A convolutional Block in the VGG16 architecture consists of 2 to 3 Conv Layers and on Pooling layer.\n",
    "You can access a models layer directly via `model.layers`.\n",
    "Read up on how to freeze layers [here](https://keras.io/guides/transfer_learning/), in case you did not use this technique in the last exercise.\n",
    "Your model should achieve a validation accuracy of close to 100% ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JYQEnwa3z23"
   },
   "source": [
    "### 1. Load data for pretraining and apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2033,
     "status": "ok",
     "timestamp": 1606772397803,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "yEDBPC7K3z23",
    "outputId": "a447e4d7-113f-4209-ede6-13192f7c629e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pictures of pokemons: 347, and  two of them just for fun of it : \n",
      "[<PIL.Image.Image image mode=RGB size=224x224 at 0x7F0B75D4C320>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F0B75D4C390>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F0B75D4C438>]\n"
     ]
    }
   ],
   "source": [
    "PATH = '/content/gdrive/MyDrive/Colab Data/pokemons/imgs/*'\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "X = [image.load_img(file, color_mode='rgb', target_size=IMAGE_SIZE, interpolation='nearest') for file in glob.glob(PATH)]\n",
    "print(\"Number of pictures of pokemons: {}, and  two of them just for fun of it : \\n{}\".format(len(X),X[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 580,
     "status": "ok",
     "timestamp": 1606772402086,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "LWtTmK113z23",
    "outputId": "d1f9a83b-f7e3-40f1-809f-2dc46a4df804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pictures: 347, \n",
      "shape of one of the pictures : \n"
     ]
    }
   ],
   "source": [
    "X_array = [image.img_to_array(x_, data_format=None, dtype=None) for x_ in X]\n",
    "\n",
    "print(\"Number of pictures: {}, \\nshape of one of the pictures : \".format(len(X_array),X_array[7].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1606772404690,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "y68KKWsF3z24",
    "outputId": "6f0ba0a3-749b-4e10-ef1d-1270fb20de93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of one of the new preprocessed pictures : \n",
      "(224, 224, 3) \n"
     ]
    }
   ],
   "source": [
    "X_preprocessed =  [preprocess_input(x_) for x_ in X_array]\n",
    "\n",
    "print(\"The shape of one of the new preprocessed pictures : \\n{} \".format(X_preprocessed[7].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1606772405739,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "uzhdqCCS3z24",
    "outputId": "9b0900a1-c52c-49f2-b0dd-a0e89526b0ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of encoded labels: (347, 8), first three of them: \n",
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "pokemons = ['bulbasaur', 'charmander', 'eevee', 'flareon', 'jolteon', 'pikachu', 'squirtle', 'vaporeon']\n",
    "\n",
    "# path is equal to ../input/pokemons/imgs/imgs/*\n",
    "y = [pokemons.index(x.split('/')[7].split('_')[0].lower()) for x in glob.glob(PATH)]\n",
    "y_encoded = tf.keras.utils.to_categorical(y, 8)\n",
    "print(\"The shape of encoded labels: {}, first three of them: \\n{}\".format(y_encoded.shape, y_encoded[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncWqGsCn3z24"
   },
   "source": [
    "### 2. Split data into training and test partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeWpMmvc3z24"
   },
   "source": [
    "As I understand, we first must ensure that input data is a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 881,
     "status": "ok",
     "timestamp": 1606772408791,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "FA7AIHvl3z24",
    "outputId": "6d581ec7-30aa-48cc-c452-5473de734076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of images: (347, 224, 224, 3), and type \n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_pred_ndarray = np.array(X_preprocessed, dtype=np.float32)\n",
    "print(\"The shape of images: {}, and type \\n{}\".format(X_pred_ndarray.shape, type(X_pred_ndarray)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 641,
     "status": "ok",
     "timestamp": 1606772410014,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "rH_ZvRgb3z24"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pred_ndarray, y_encoded, train_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvRlex6M3z24"
   },
   "source": [
    "\n",
    "\n",
    "### 3. Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7259,
     "status": "ok",
     "timestamp": 1606772417883,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "hGLlsFo23z24",
    "outputId": "0f8508a0-e4d0-4df7-a37b-de05e2644227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "base_vgg = VGG16(weights=\"imagenet\", include_top = False, input_shape=(224, 224, 3))\n",
    "base_vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5KzucK-3z24"
   },
   "source": [
    "### 4. Freeze weights and perform training step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1606772419718,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "EonTJdLS3z24",
    "outputId": "fcc0d516-4470-4846-ea12-38ae820cfa93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_vgg.trainable = False\n",
    "\n",
    "base_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1606772420811,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "lOMaX0C93z24",
    "outputId": "50240c1e-b682-40e6-882a-b271e4266ae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "outupt_eight (Dense)         (None, 8)                 200712    \n",
      "=================================================================\n",
      "Total params: 14,915,400\n",
      "Trainable params: 200,712\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ = Input(shape=(224, 224, 3))\n",
    "\n",
    "concat_ = base_vgg(input_, training=False)\n",
    "\n",
    "vectors_ = Flatten()(concat_)\n",
    "\n",
    "output_ = Dense(8, activation='softmax', name = 'outupt_eight')(vectors_)\n",
    "\n",
    "model_vgg = Model(input_, output_)\n",
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17329,
     "status": "ok",
     "timestamp": 1606772438072,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "QRw2pwgJ3z25",
    "outputId": "eaa7d7ec-f688-473c-b5c1-f11a6ef364cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 3s 274ms/step - loss: 18.4990 - acc: 0.4647 - val_loss: 6.1892 - val_acc: 0.7429\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 2.1833 - acc: 0.9071 - val_loss: 5.2614 - val_acc: 0.8000\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.0412 - acc: 0.9968 - val_loss: 6.6279 - val_acc: 0.8286\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.0993 - acc: 0.9936 - val_loss: 7.0155 - val_acc: 0.8286\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 1.4166e-06 - acc: 1.0000 - val_loss: 6.9978 - val_acc: 0.8286\n"
     ]
    }
   ],
   "source": [
    "model_vgg.compile(optimizer='adam', loss='categorical_crossentropy', metrics='acc')\n",
    "\n",
    "history = model_vgg.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Tx_WxqL3z25"
   },
   "source": [
    "### 5. Unfreeze weights and perform training step 2\n",
    "\n",
    "Task is: \n",
    "* Unfreeze the last two convolutional Blocks\n",
    "* Continue training with the following settings:\n",
    "    * 10 Epochs\n",
    "    * Adam Optimizer with a learning rate of 0.0001\n",
    "    * Batchsize of 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4h-PZ_m3z25"
   },
   "source": [
    "Base model has 19 layers. The last one in it is MaxPooling and two before MaxPooling are Conv2d - exactly what we must set to trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1606772444769,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "zEHDT4IT3z25",
    "outputId": "e560a03f-b4e8-4194-e9c1-722a596c9571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 4,719,616\n",
      "Non-trainable params: 9,995,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_vgg.trainable = True\n",
    "for layer in base_vgg.layers[0:16]:\n",
    "    layer.trainable =  False\n",
    "    \n",
    "base_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15518,
     "status": "ok",
     "timestamp": 1606772465392,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "H7TAj9h_3z25",
    "outputId": "d15e7f24-e57b-456c-83bc-06cd105994c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "outupt_eight (Dense)         (None, 8)                 200712    \n",
      "=================================================================\n",
      "Total params: 14,915,400\n",
      "Trainable params: 4,920,328\n",
      "Non-trainable params: 9,995,072\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 2s 152ms/step - loss: 1.0813e-07 - acc: 1.0000 - val_loss: 8.9200 - val_acc: 0.8286\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 6.4954e-09 - acc: 1.0000 - val_loss: 9.9819 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 10.2965 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 1.0966e-07 - acc: 1.0000 - val_loss: 10.9063 - val_acc: 0.7714\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 2.7098e-04 - acc: 1.0000 - val_loss: 11.4790 - val_acc: 0.7714\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.1011 - acc: 0.9904 - val_loss: 6.8487 - val_acc: 0.8857\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.1055 - acc: 0.9904 - val_loss: 4.3838 - val_acc: 0.8857\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 5.4626 - val_acc: 0.8000\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 0.3633 - acc: 0.9808 - val_loss: 6.6962 - val_acc: 0.8571\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.2835 - acc: 0.9808 - val_loss: 5.6606 - val_acc: 0.8857\n"
     ]
    }
   ],
   "source": [
    "model_vgg.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics='acc')\n",
    "\n",
    "model_vgg.summary()\n",
    "\n",
    "history = model_vgg.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDcWlXLq3z25"
   },
   "source": [
    "## 2. Pretrain the model\n",
    "Now that we have our initial model we are going to extend it with some more pokemon.\n",
    "[Here](https://megastore.uni-augsburg.de/get/OxpI3M_JyU/) you will find roughly 6000 images of the following Pokemon:\n",
    "* Blastoise\n",
    "* Charizard\n",
    "* Charmeleon\n",
    "* Ivysaur\n",
    "* Venusaur\n",
    "* Wartortle\n",
    "\n",
    "Unfortunately images are not labeled yet. To speed things up a bit we are only going to label a small part of the data ourselves, and then build a model to help us doing the rest.\n",
    "(Actually this will probably not be faster, but more fun anyway :) ).\n",
    "In your project directory you will find a 'data_labled' folder, which we will use to store the labeled data.\n",
    "This time we will use the folder structure to create our labels and train / validation partitions.\n",
    "Inside the folder you will therefore find a 'train' and an 'val' folder, each of them containing subfolders for each class.\n",
    "\n",
    "In the following step you should at first manually pick at least 5 examples per class and copy them from the 'data' folder to the train partition of the 'data_labeled' folder.\n",
    "To then take full advantage of the current way the data is structured, we will use keras data generators in combination with the `flow_from_directory` to dynamically read the input data and feed it to our model.\n",
    "You can find an example of such data generators [here](https://keras.io/api/preprocessing/image/#flowfromdirectory-method).\n",
    "\n",
    "Specifically we are going to write a function `train_loop()` which creates two data generators (one for training and one for validation) and trains a model for the new Images on features extracted from our current Pokexedx model.\n",
    "To this end you can simply rebuild the structure of the original model, but replace the number of output classes.\n",
    "To load the weights you can then use the following code snippet:\n",
    "`model.layers[-1]._name = 'new_output'`</br>\n",
    "`model.load_weights(weight_path, by_name=True)`</br>\n",
    "\n",
    "Freeze all layers but the dense layers, we will only need those and want to speed up the training process a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4hqpsXJ3z25"
   },
   "source": [
    "### Saving model(for fun) and its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 780,
     "status": "ok",
     "timestamp": 1606772469480,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "xxbnV8my3z25"
   },
   "outputs": [],
   "source": [
    "model_vgg.save('model_vgg-pokemons_on_imgs.h5')\n",
    "\n",
    "model_vgg.save_weights('model_vgg-pokemons_on_imgs_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb5HMxmm3z25"
   },
   "source": [
    "### 6. Copy at least 5 images per class from the data folder to the correct partition in the data_labeled folder\n",
    "\n",
    "*DONE!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7dDjlo53z25"
   },
   "source": [
    "### 7. Function train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 777,
     "status": "ok",
     "timestamp": 1606772472661,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "rc27Ql0g3z25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    keras.backend.clear_session()\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # 8. Build model\n",
    "    input_ = Input(shape=(224, 224, 3))\n",
    "\n",
    "    concat_ = base_vgg(input_, training=False)\n",
    "\n",
    "    vectors_ = Flatten()(concat_)\n",
    "\n",
    "    output_ = Dense(6, activation='softmax', name = 'output_six')(vectors_)\n",
    "\n",
    "    model = Model(input_, output_)\n",
    "    \"\"\"\n",
    "    print(\"Model's summary BEFORE loading weights:\") \n",
    "    model.summary()\n",
    "    print(\"BASE Model's summary BEFORE loading weights:\")\n",
    "    base_vgg.summary()\n",
    "    \"\"\"\n",
    "    # 9. Load weights\n",
    "    print('LOADING WEIGHTS . . .')\n",
    "    model.load_weights('model_vgg-pokemons_on_imgs_weights.hdf5', by_name = True)\n",
    "    \"\"\"\n",
    "    print(\"Model's summary AFTER loading weights: \")\n",
    "    model.summary()\n",
    "    print(\"BASE Model's summary AFTER loading weights:\")\n",
    "    base_vgg.summary()\n",
    "    \"\"\"\n",
    "    # 10. Build data generators\n",
    "    train_data_dir = '/content/gdrive/MyDrive/Colab Data/pokemons/data_labeled/train'\n",
    "    val_data_dir = '/content/gdrive/MyDrive/Colab Data/pokemons/data_labeled/val'\n",
    "\n",
    "    train_datagen = image.ImageDataGenerator()\n",
    "    val_datagen = image.ImageDataGenerator()\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory = train_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=16)\n",
    "\n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "        directory = val_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=16)\n",
    "    \n",
    "    # 11. Fit the model to the data for a few epochs\n",
    "    model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics='acc')\n",
    "    \n",
    "    model.fit_generator(train_generator, validation_data=validation_generator, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIQ_YCsp3z25"
   },
   "source": [
    "### 12. Call train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67739,
     "status": "ok",
     "timestamp": 1606772541839,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "_s1dR46x3z25",
    "outputId": "40ebb15f-03ad-4339-f723-ad810dc37c62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING WEIGHTS . . .\n",
      "Found 30 images belonging to 6 classes.\n",
      "Found 90 images belonging to 6 classes.\n",
      "WARNING:tensorflow:From <ipython-input-28-a9efea354d5c>:51: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 47s 23s/step - loss: 31.5176 - acc: 0.1667 - val_loss: 22.9393 - val_acc: 0.2333\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 2.6575 - acc: 0.8333 - val_loss: 21.3930 - val_acc: 0.2889\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.2865 - acc: 0.9667 - val_loss: 18.1234 - val_acc: 0.2889\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 8.7946e-04 - acc: 1.0000 - val_loss: 15.5587 - val_acc: 0.3222\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 13.8663 - val_acc: 0.3889\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 3.8415e-05 - acc: 1.0000 - val_loss: 12.9300 - val_acc: 0.4333\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 388ms/step - loss: 6.8657e-06 - acc: 1.0000 - val_loss: 12.3043 - val_acc: 0.4333\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 2.0583e-06 - acc: 1.0000 - val_loss: 11.8675 - val_acc: 0.4444\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 9.9340e-07 - acc: 1.0000 - val_loss: 11.5550 - val_acc: 0.4333\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 1s 332ms/step - loss: 6.1591e-07 - acc: 1.0000 - val_loss: 11.3309 - val_acc: 0.4444\n"
     ]
    }
   ],
   "source": [
    "train_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1euk-2YQ3z25"
   },
   "source": [
    "## 3. Interactive UI\n",
    "\n",
    "In this part of the exercise we are going to put our pretrained model to good use by employing it in a cooperative workflow.\n",
    "To this end we gonna build a minimal cooperative machine learning using interface in this python notebook.\n",
    "Our user interface will consist of the following components:\n",
    "\n",
    "* (optional) A progressbar to keep to motivation up\n",
    "* A slider to set a high confidence threshold\n",
    "* A slider to set the mid confidence threshold\n",
    "* Some radio buttons to choose the label\n",
    "* A button to save the annotation and label and show the next image\n",
    "* A button to retrain our model\n",
    "* A button to use our model to predict our dataset\n",
    "\n",
    "The final our UI should look a like this:\n",
    "\n",
    "![img](https://hcm-lab.de/cloud/index.php/s/ak3txGXepnt9NxS/preview)\n",
    "\n",
    "The 'retrain' button should call the `train_loop()`  function from before to retrain the model on all labeled data.\n",
    "The 'predict' button should create a list of predictions for all unlabeled images.\n",
    "All predictions that are above the high confidence threshold, set by the respective slider, should be automatically accepted as correct label and copied to the respective folders in the training data folder.\n",
    "Additionally you should implement a garbage label to delete unfitting images.\n",
    "Potential reasons to consider an Image as garbage are if no Pokemon is visible, too many Pokemon are visible, non of the Pokemon we want to train are visible, the Imagefile is broken etc.\n",
    "When you are pressing the 'next' button the current image should be copied to the right folder in the training dataset, depending on the current value of the radio button.\n",
    "Afterwards the next image should be chosen from all predicted images, where the confidence is greater or equal than the value set by the mid_threshold slider.\n",
    "The current value of the radiobutton should then be set to the prediction for this respective image.\n",
    "Optionally you can also implement a progressbar to track your progress for you annotations.\n",
    "\n",
    "You can use the ipywidgets library to create the UI.\n",
    "You can find an IPython tutorial [here](https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6) and the api documentation [here](https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6).\n",
    "Note, that Pycharm might not play well with the the widgets in all scenarios. It's best to view them in the browser by visting: http://localhost:8888 after you started your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA5jJfOb3z25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### 13. Build UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0WQawkpIhbF"
   },
   "source": [
    "#### Methods to create widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1606775632506,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "teu0f8OhIgOS"
   },
   "outputs": [],
   "source": [
    "def create_predict_button():    \n",
    "  button_predict = widgets.Button(description='Predict', button_style='success')\n",
    "  button_predict.on_click(on_predict)\n",
    "  return button_predict\n",
    "\n",
    "def create_retrain_button():\n",
    "  button_retrain = widgets.Button(description='Retrain', button_style='danger')\n",
    "  button_retrain.on_click(on_retrain)\n",
    "  return button_retrain\n",
    "\n",
    "def create_next_button():\n",
    "  button_next = widgets.Button(description='Next', button_style='success')\n",
    "  button_next.on_click(on_next)\n",
    "  return button_next\n",
    "\n",
    "labels = widgets.RadioButtons(options=['Blastoise', 'Charizard', 'Charmeleon', 'Ivysaur', 'Venusaur', 'Wartortle'],  \n",
    "                                description='Labels:', disabled=False)\n",
    "\n",
    "def create_labels_radio():\n",
    "  return labels\n",
    "\n",
    "high_conf = widgets.IntSlider(value=80, min=0, max=100, step=5, description='High Conf:', disabled=False, continuous_update=False,\n",
    "    orientation='horizontal', readout=True, readout_format='d')\n",
    "\n",
    "def create_high_conf():\n",
    "  return high_conf\n",
    "\n",
    "mid_conf = widgets.IntSlider(value=50, min=0, max=100, step=5, description='Mid Conf:', disabled=False, continuous_update=False,\n",
    "    orientation='horizontal', readout=True, readout_format='d')\n",
    "\n",
    "def create_mid_conf():\n",
    "  return mid_conf\n",
    "\n",
    "path = '/content/gdrive/MyDrive/Colab Data/pokemons/data/'\n",
    "\n",
    "def create_image():\n",
    "  \"\"\"file=os.listdir(path)\n",
    "  display(Image(path+file))\"\"\"\n",
    "  \n",
    "  file = open(path, \"rb\")\n",
    "  image = file.read()\n",
    "  return widgets.Image(value=image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5SqKIy2Hy78"
   },
   "source": [
    "#### Collable methods from widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 644,
     "status": "ok",
     "timestamp": 1606775634633,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "marl5X799-OI"
   },
   "outputs": [],
   "source": [
    "def on_predict(r):\n",
    "  print('Predicting ...', r)\n",
    "\n",
    "def on_retrain(r):\n",
    "  train_loop()\n",
    "\n",
    "def on_next(label=''):\n",
    "  print('Next ...', labels.value)\n",
    "\n",
    "def on_label():\n",
    "  print('{} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8U5SRECFSqPE"
   },
   "outputs": [],
   "source": [
    "path = '/content/gdrive/MyDrive/Colab Data/pokemons/data/*'\n",
    "\n",
    "X = [image.load_img(file, target_size=IMAGE_SIZE, interpolation='nearest') for file in glob.glob(path)]\n",
    "X_array = [image.img_to_array(x_, data_format=None, dtype=None) for x_ in X]\n",
    "X_preprocessed =  [preprocess_input(x_) for x_ in X_array]\n",
    "X_test = np.array(X_preprocessed, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPc-BZCqEpOQ"
   },
   "source": [
    "#### Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617,
     "referenced_widgets": [
      "7232e7cf9a824f0182c3783c3bce6c69",
      "3a751d9b497d48a3b51a04c9d7ce0ca1",
      "88067d9a645945948f83fbb0b2f9b51b",
      "c447f124633c465381242c778caf119f",
      "ffe71a2ce3c8464c96c08095ad39f229",
      "6ffd51c45ff34600b6f2cdf85d2cebc3",
      "6e38ab04b7694a3b896dc6984af9c7f1",
      "e116f815c95e49b6aa0f0b68d51b5278",
      "a4ce8a1337e44f97a39326476664d3f8",
      "388861ca497644e78c3163471a834b2b",
      "5ebdaf62c4a04bc1814d97cb74f5d92a",
      "8e99bb1bcc2b4829b8e1d178b3999c2e",
      "d1fbdd880a5b46ec9707f1777be5fcdb",
      "52a015e139084064bf38a09f08afba94",
      "2181fc9a5f8c46a788f261abe2a92a45",
      "c7548865db1348b68abe5acebd2bcab1",
      "711848ce9c154ea1b7d0b7f82625950a",
      "25e12032e51b48ec81bec02b969c6663",
      "dbe74b6dc53543b997410abea9fa9cb2",
      "fe0e5fbe3b5542b5a6370ce002cbdea4",
      "b6f3579790ff47abadc56144ee22e230",
      "fc5a9f969d014008853fe760e2e26408"
     ]
    },
    "executionInfo": {
     "elapsed": 753,
     "status": "ok",
     "timestamp": 1606775636610,
     "user": {
      "displayName": "Paul Mospan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gje-6RRBkrERIx7x0uSooTOdBI1tD7jeOCcJiH8=s64",
      "userId": "13717733502475344631"
     },
     "user_tz": -60
    },
    "id": "hvvLu4Xe3z26",
    "outputId": "30a82fee-f981-49f6-d237-44e0897de94d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7232e7cf9a824f0182c3783c3bce6c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(IntSlider(value=80, continuous_update=False, description='High Conf:', layout=Layout(…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import GridspecLayout\n",
    "\n",
    "grid = GridspecLayout(9, 4, height='600px')\n",
    "grid[1, 0] = create_high_conf()\n",
    "grid[2, 0] = create_mid_conf()\n",
    "grid[3, 0] = create_labels_radio()\n",
    "grid[5, 0] = create_next_button()\n",
    "grid[7, 0] = create_retrain_button()\n",
    "grid[8, 0] = create_predict_button()\n",
    "grid[:, 1:] = create_image()\n",
    "\n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LmLT9V23z26",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. repeat(annotate, train, predict)\n",
    "After you are done creating the UI, we are now going to label the whole dataset together with our model.\n",
    "To this end use your model to predict and improve iteratively in the following manner:\n",
    "\n",
    "Set the high confidence slider to a value greater or equal than 0.95 and the mid confidence slider to at least 0.8\n",
    "\n",
    "Repeat 3 times:\n",
    "\n",
    "* Call automatic prediction\n",
    "* Check images that have been above the maximum confidence threshold manually by looking at the content of the respective folders. Make corrections if necessary.\n",
    "* Annotate remaining images that have been over the mid confidence score\n",
    "* Retrain you model\n",
    "\n",
    "Do you notice any change in the amount of images you have to annotate each time?\n",
    "\n",
    "Repeat till all data is annotated:\n",
    "\n",
    "* Call automatic prediction\n",
    "* Annotate remaining images that have been over the mid confidence score\n",
    "* Retrain you model\n",
    "* Adjust both confidence scores based on how much you trust your model\n",
    "\n",
    "Describe your subjective impression of the annotation process. Did you have the feeling, that the cooperative workflow is helpful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6_yfImgT3T7"
   },
   "source": [
    "ANSWER:  It is a very helpful tool to manually annotate images that model cannot assign class with desired confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzx16XPuT8_L"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "interactive-ml-04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2181fc9a5f8c46a788f261abe2a92a45": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": "widget003",
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25e12032e51b48ec81bec02b969c6663": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "388861ca497644e78c3163471a834b2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "3a751d9b497d48a3b51a04c9d7ce0ca1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": "\". widget007 widget007 widget007\"\n\"widget001 widget007 widget007 widget007\"\n\"widget002 widget007 widget007 widget007\"\n\"widget003 widget007 widget007 widget007\"\n\". widget007 widget007 widget007\"\n\"widget004 widget007 widget007 widget007\"\n\". widget007 widget007 widget007\"\n\"widget005 widget007 widget007 widget007\"\n\"widget006 widget007 widget007 widget007\"",
      "grid_template_columns": "repeat(4, 1fr)",
      "grid_template_rows": "repeat(9, 1fr)",
      "height": "600px",
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52a015e139084064bf38a09f08afba94": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ebdaf62c4a04bc1814d97cb74f5d92a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": "widget001",
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e38ab04b7694a3b896dc6984af9c7f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "danger",
      "description": "Retrain",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_dbe74b6dc53543b997410abea9fa9cb2",
      "style": "IPY_MODEL_25e12032e51b48ec81bec02b969c6663",
      "tooltip": ""
     }
    },
    "6ffd51c45ff34600b6f2cdf85d2cebc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "success",
      "description": "Next",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_711848ce9c154ea1b7d0b7f82625950a",
      "style": "IPY_MODEL_c7548865db1348b68abe5acebd2bcab1",
      "tooltip": ""
     }
    },
    "711848ce9c154ea1b7d0b7f82625950a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": "widget004",
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7232e7cf9a824f0182c3783c3bce6c69": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "GridBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "GridBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "GridBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_88067d9a645945948f83fbb0b2f9b51b",
       "IPY_MODEL_c447f124633c465381242c778caf119f",
       "IPY_MODEL_ffe71a2ce3c8464c96c08095ad39f229",
       "IPY_MODEL_6ffd51c45ff34600b6f2cdf85d2cebc3",
       "IPY_MODEL_6e38ab04b7694a3b896dc6984af9c7f1",
       "IPY_MODEL_e116f815c95e49b6aa0f0b68d51b5278",
       "IPY_MODEL_a4ce8a1337e44f97a39326476664d3f8"
      ],
      "layout": "IPY_MODEL_3a751d9b497d48a3b51a04c9d7ce0ca1"
     }
    },
    "88067d9a645945948f83fbb0b2f9b51b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": false,
      "description": "High Conf:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_5ebdaf62c4a04bc1814d97cb74f5d92a",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 5,
      "style": "IPY_MODEL_388861ca497644e78c3163471a834b2b",
      "value": 80
     }
    },
    "8e99bb1bcc2b4829b8e1d178b3999c2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "a4ce8a1337e44f97a39326476664d3f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "png",
      "height": "",
      "layout": "IPY_MODEL_fc5a9f969d014008853fe760e2e26408",
      "width": ""
     }
    },
    "b6f3579790ff47abadc56144ee22e230": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": "widget006",
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c447f124633c465381242c778caf119f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": false,
      "description": "Mid Conf:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_d1fbdd880a5b46ec9707f1777be5fcdb",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 5,
      "style": "IPY_MODEL_8e99bb1bcc2b4829b8e1d178b3999c2e",
      "value": 50
     }
    },
    "c7548865db1348b68abe5acebd2bcab1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "d1fbdd880a5b46ec9707f1777be5fcdb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": "widget002",
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbe74b6dc53543b997410abea9fa9cb2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": "widget005",
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e116f815c95e49b6aa0f0b68d51b5278": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "success",
      "description": "Predict",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_b6f3579790ff47abadc56144ee22e230",
      "style": "IPY_MODEL_fe0e5fbe3b5542b5a6370ce002cbdea4",
      "tooltip": ""
     }
    },
    "fc5a9f969d014008853fe760e2e26408": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": "widget007",
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe0e5fbe3b5542b5a6370ce002cbdea4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "ffe71a2ce3c8464c96c08095ad39f229": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "RadioButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "RadioButtonsModel",
      "_options_labels": [
       "Blastoise",
       "Charizard",
       "Charmeleon",
       "Ivysaur",
       "Venusaur",
       "Wartortle"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "RadioButtonsView",
      "description": "Labels:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_2181fc9a5f8c46a788f261abe2a92a45",
      "style": "IPY_MODEL_52a015e139084064bf38a09f08afba94"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
